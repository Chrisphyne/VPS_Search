## 8. Summary and Next Steps

### ✅ What We've Built:

1. **Field Schema Manager** - Automatically extracts and understands your dynamic field schemas
2. **Smart JSONB Query Builder** - Creates intelligent SQL queries for JSONB data
3. **Data Processor** - Formats occurrence data for LLM consumption
4. **Vector Store** - Enables semantic search on occurrence content
5. **Enhanced RAG System** - Provides intelligent, schema-aware responses

### 🔥 Key Benefits:

- **Schema Awareness**: Your LLM understands what fields exist for each occurrence type
- **JSONB Intelligence**: Properly extracts and interprets data from your flexible JSONB columns
- **Semantic Search**: Find similar occurrences even with different wording
- **Field-Specific Queries**: Ask about available fields and requirements for any occurrence type
- **Natural Language Interface**: Query your occurrence data in plain English

### 🚀 Usage Examples:

```python
# Query for specific incidents
query_occurrences("What vehicle thefts happened at Sarit Center?")

# Ask about field schemas  
query_occurrences("What information is required for reporting arson?")

# Search for patterns
query_occurrences("Show me all death cases caused by accidents")

# Get field options
query_occurrences("What are the available options for cyber crime types?")
```

### 📈 Performance Optimizations:

For production use, consider:
- Creating GIN indexes on JSONB columns: `CREATE INDEX idx_formdata_gin ON sub_module_data USING GIN ("formData");`
- Implementing caching for frequent queries
- Using connection pooling for database connections
- Batch processing for large vector store updates

Your JSONB occurrence data is now intelligently available to your LLM! 🎉def query_occurrences(question: str):
    """
    Simple function to query your occurrence data
    
    Examples:
    - query_occurrences("What vehicle thefts happened at Sarit Center?")
    - query_occurrences("Show me recent fire incidents")
    - query_occurrences("What fields are available for death reports?")
    """
    try:
        answer = enhanced_rag.answer_question(question)
        return answer
    except Exception as e:
        return f"Error processing question: {e}"

# Example usage
print("Example Queries:")
print("=" * 30)

example_questions = [
    "What vehicle thefts happened recently?",
    "Show me death cases from accidents"
]

for q in example_questions:
    print(f"\\nQ: {q}")
    print(f"A: {query_occurrences(q)[:200]}...")

print("\\n\\nFunction 'query_occurrences(question)' is ready for use!")## 7. Interactive Query Function

Simple function to interactively query your occurrence data.# Test field-specific schema queries
schema_questions = [
    "What information is collected for Motor Vehicle Theft cases?",
    "What fields are required when reporting an Arson incident?", 
    "Show me the available options for Cyber Crime incidents"
]

print("Testing Field Schema Understanding:")
print("=" * 50)

for question in schema_questions:
    print(f"\\nQ: {question}")
    print("-" * 30)
    answer = enhanced_rag.answer_question(question)
    print(f"A: {answer[:250]}{'...' if len(answer) > 250 else ''}")

# Direct schema exploration
print("\\n\\nDirect Schema Exploration:")
print("=" * 40)

# Show detailed schema for Motor Vehicle Theft (ID 8)
motor_vehicle_schema = schema_manager.create_field_description(8)
print("Motor Vehicle Theft Fields:")
print(motor_vehicle_schema[:500] + "..." if len(motor_vehicle_schema) > 500 else motor_vehicle_schema)## 6. Field-Specific Queries

Demonstrate how the system can answer field-specific questions about your occurrence schemas.# Test the Enhanced RAG System
test_questions = [
    "What types of vehicle theft have been reported recently?",
    "Tell me about arson cases and what property types are affected",
    "What fields are available when reporting a missing person?",
    "How many death cases have been reported?",
    "What are the common locations for theft incidents?"
]

print("Testing Enhanced RAG System")
print("=" * 60)

for i, question in enumerate(test_questions, 1):
    print(f"\\n{i}. Question: {question}")
    print("-" * 50)
    
    try:
        answer = enhanced_rag.answer_question(question)
        print(f"Answer: {answer[:300]}{'...' if len(answer) > 300 else ''}")
    except Exception as e:
        print(f"Error: {e}")
    
    print("\\n" + "="*60)class EnhancedOccurrenceRAG:
    """Enhanced RAG system for occurrence data with schema awareness"""
    
    def __init__(self, llm, vector_store: OccurrenceVectorStore, schema_manager: FieldSchemaManager, query_builder: JSONBQueryBuilder):
        self.llm = llm
        self.vector_store = vector_store
        self.schema_manager = schema_manager
        self.query_builder = query_builder
        self.setup_prompts()
    
    def setup_prompts(self):
        """Setup prompts for different types of queries"""
        
        # General RAG prompt with schema awareness
        self.rag_prompt = PromptTemplate.from_template("""
You are an intelligent assistant analyzing police occurrence reports. You have access to:
1. Occurrence data with structured fields
2. Field schemas that define what information is available
3. Historical occurrence patterns

Available Occurrence Types and Their Fields:
{schema_info}

Context from similar occurrences:
{context}

Question: {question}

Instructions:
- Provide accurate information based on the occurrence data
- Mention specific OB numbers when referencing occurrences
- Explain what fields are available for different occurrence types
- If asked about trends, analyze patterns in the data
- If information is not available, clearly state this

Answer:
""")
    
    def get_schema_summary(self) -> str:
        """Get summary of all available schemas"""
        summary = []
        for module_id, schema in self.schema_manager.schemas.items():
            summary.append(f"{schema['name']}: {schema['description']}")
        return "\\n".join(summary)
    
    def answer_question(self, question: str) -> str:
        """Answer question using RAG approach"""
        # Get relevant documents
        relevant_docs = self.vector_store.search_similar_occurrences(question)
        
        # Format context
        context = "\\n\\n".join([doc.page_content for doc in relevant_docs])
        
        # Get schema info
        schema_info = self.get_schema_summary()
        
        # Generate answer
        prompt = self.rag_prompt.format(
            schema_info=schema_info,
            context=context,
            question=question
        )
        
        response = self.llm.invoke(prompt)
        return response.content

# Initialize enhanced RAG system
enhanced_rag = EnhancedOccurrenceRAG(llm, vector_store, schema_manager, query_builder)
print("✅ Enhanced Occurrence RAG System initialized!")## 5. Enhanced RAG System

Create the complete RAG system that understands field schemas and can provide intelligent answers.# Test semantic search capabilities
test_queries = [
    "vehicle theft at sarit center",
    "death due to accident", 
    "stolen laptop",
    "fire incidents"
]

print("Testing Semantic Search:")
print("=" * 50)

for query in test_queries:
    print(f"\\nSearching for: '{query}'")
    print("-" * 30)
    
    results = vector_store.search_similar_occurrences(query, k=2)
    
    for i, doc in enumerate(results, 1):
        print(f"\\nResult {i}:")
        print(f"OB Number: {doc.metadata.get('ob_number', 'N/A')}")
        print(f"Type: {doc.metadata.get('module_name', 'N/A')}")
        print(f"Content: {doc.page_content[:150]}...")
        print("-" * 25)# Load occurrences into vector store (this may take a few moments)
vector_store.load_occurrences_to_vectorstore(limit=30)  # Start with 30 for testingclass OccurrenceVectorStore:
    """Manage vector store for occurrence data"""
    
    def __init__(self, embeddings, data_processor: OccurrenceDataProcessor, query_builder: JSONBQueryBuilder):
        self.embeddings = embeddings
        self.data_processor = data_processor
        self.query_builder = query_builder
        self.vectorstore = None
        self.retriever = None
    
    def load_occurrences_to_vectorstore(self, limit: int = 50):
        """Load occurrences into vector store"""
        print(f"Loading {limit} recent occurrences into vector store...")
        
        # Get occurrence data
        query = self.query_builder.build_occurrence_query(limit=limit)
        results = self.query_builder.execute_query(query)
        
        if not results:
            print("No occurrence data found")
            return
        
        # Parse results
        occurrences = parse_db_result_to_dict(results)
        print(f"Parsed {len(occurrences)} occurrences")
        
        # Create documents
        documents = self.data_processor.create_occurrence_documents(occurrences)
        print(f"Created {len(documents)} documents")
        
        # Split documents if they're too long
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        split_docs = text_splitter.split_documents(documents)
        print(f"Split into {len(split_docs)} chunks")
        
        # Create vector store
        self.vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory="./occurrence_vectorstore"
        )
        
        # Create retriever
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        )
        
        print("✅ Vector store created successfully!")
    
    def search_similar_occurrences(self, query: str, k: int = 5) -> List[Document]:
        """Search for similar occurrences"""
        if not self.retriever:
            print("Vector store not initialized. Call load_occurrences_to_vectorstore first.")
            return []
        
        return self.retriever.invoke(query)

# Initialize vector store
vector_store = OccurrenceVectorStore(embeddings, data_processor, query_builder)
print("✅ Occurrence Vector Store initialized!")## 4. Vector Store for Semantic Search

Create a vector store from occurrence data for semantic search capabilities.# Test data processing with sample occurrences
def parse_db_result_to_dict(db_result_string: str) -> List[Dict]:
    """Parse database result string into list of dictionaries"""
    import ast
    try:
        # Parse the string representation of tuples
        rows = ast.literal_eval(db_result_string)
        
        # Define column names based on our query
        columns = ['id', 'ob_number', 'submissionDate', 'sub_moduleId', 'module_name', 
                  'module_description', 'formData', 'location', 'urgency', 'narrative',
                  'first_name', 'last_name', 'id_no']
        
        result = []
        for row in rows:
            occurrence_dict = dict(zip(columns, row))
            result.append(occurrence_dict)
        
        return result
    except Exception as e:
        print(f"Error parsing result: {e}")
        return []

# Get sample data and process it
sample_query = query_builder.build_occurrence_query(limit=2)
sample_results = query_builder.execute_query(sample_query)

if sample_results:
    sample_occurrences = parse_db_result_to_dict(sample_results)
    
    if sample_occurrences:
        print("Sample processed occurrence:")
        print("=" * 50)
        formatted_sample = data_processor.format_occurrence_for_llm(sample_occurrences[0])
        print(formatted_sample)
    else:
        print("❌ No sample data to process")
else:
    print("❌ No sample data available")class OccurrenceDataProcessor:
    """Process occurrence data for LLM consumption"""
    
    def __init__(self, schema_manager: FieldSchemaManager):
        self.schema_manager = schema_manager
    
    def format_occurrence_for_llm(self, occurrence_data: Dict) -> str:
        """Format occurrence data into human-readable text for LLM"""
        formatted = []
        
        # Basic occurrence info
        formatted.append(f"OB Number: {occurrence_data.get('ob_number', 'N/A')}")
        formatted.append(f"Date: {occurrence_data.get('submissionDate', 'N/A')}")
        formatted.append(f"Type: {occurrence_data.get('module_name', 'N/A')}")
        formatted.append(f"Description: {occurrence_data.get('module_description', 'N/A')}")
        
        if occurrence_data.get('location'):
            formatted.append(f"Location: {occurrence_data['location']}")
        
        if occurrence_data.get('urgency'):
            formatted.append(f"Urgency: {occurrence_data['urgency']}")
        
        # Person information
        if occurrence_data.get('first_name'):
            name = f"{occurrence_data.get('first_name', '')} {occurrence_data.get('last_name', '')}".strip()
            formatted.append(f"Reporter: {name}")
        
        if occurrence_data.get('id_no'):
            formatted.append(f"ID Number: {occurrence_data['id_no']}")
        
        # Process JSONB form data
        form_data = occurrence_data.get('formData', {})
        if isinstance(form_data, str):
            try:
                form_data = json.loads(form_data)
            except:
                form_data = {}
        
        if form_data:
            formatted.append("\\nOccurrence Details:")
            
            for key, value in form_data.items():
                if value and str(value).strip() and str(value) != 'null':
                    # Clean up the key name
                    clean_key = key.replace('_', ' ').title()
                    formatted.append(f"- {clean_key}: {value}")
        
        if occurrence_data.get('narrative'):
            formatted.append(f"\\nNarrative: {occurrence_data['narrative']}")
        
        return "\\n".join(formatted)
    
    def create_occurrence_documents(self, occurrences: List[Dict]) -> List[Document]:
        """Create LangChain documents from occurrence data"""
        documents = []
        
        for occurrence in occurrences:
            content = self.format_occurrence_for_llm(occurrence)
            
            metadata = {
                'ob_number': occurrence.get('ob_number', ''),
                'module_name': occurrence.get('module_name', ''),
                'submission_date': str(occurrence.get('submissionDate', '')),
                'urgency': occurrence.get('urgency', ''),
                'location': occurrence.get('location', '')
            }
            
            documents.append(Document(
                page_content=content,
                metadata=metadata
            ))
        
        return documents

# Initialize data processor
data_processor = OccurrenceDataProcessor(schema_manager)
print("✅ Occurrence Data Processor initialized!")## 3. Occurrence Data Processor

This processes occurrence data and formats it for LLM consumption with proper field interpretation.# Test the query builder with recent occurrences
print("Testing Query Builder:")
print("=" * 40)

# Get recent occurrences
recent_query = query_builder.build_occurrence_query(limit=3)
recent_results = query_builder.execute_query(recent_query)

if recent_results:
    print("✅ Successfully retrieved recent occurrences")
    print(f"Sample result (truncated): {str(recent_results)[:200]}...")
else:
    print("❌ No results returned")

# Search for specific terms
print("\\nSearching for 'stolen' in occurrences:")
search_query = query_builder.search_in_jsonb("stolen", limit=2)
search_results = query_builder.execute_query(search_query)

if search_results:
    print("✅ Found occurrences containing 'stolen'")
    print(f"Sample result: {str(search_results)[:200]}...")
else:
    print("❌ No 'stolen' occurrences found")class JSONBQueryBuilder:
    """Build intelligent SQL queries for JSONB data"""
    
    def __init__(self, db: SQLDatabase, schema_manager: FieldSchemaManager):
        self.db = db
        self.schema_manager = schema_manager
    
    def build_occurrence_query(self, limit: int = 10, module_id: Optional[int] = None) -> str:
        """Build query to get occurrence data with schema information"""
        base_query = """
        SELECT 
            smd.id,
            smd.ob_number,
            smd."submissionDate",
            smd."sub_moduleId",
            sm.name as module_name,
            sm.description as module_description,
            smd."formData",
            smd.location,
            smd.urgency,
            smd.narrative,
            ip.first_name,
            ip.last_name,
            ip.id_no
        FROM sub_module_data smd
        LEFT JOIN sub_module sm ON smd."sub_moduleId" = sm.id
        LEFT JOIN "IPRS_Person" ip ON smd."iprsId" = ip.id
        """
        
        if module_id:
            base_query += f" WHERE smd.\\"sub_moduleId\\" = {module_id}"
        
        base_query += f" ORDER BY smd.\\"submissionDate\\" DESC LIMIT {limit}"
        
        return base_query
    
    def search_in_jsonb(self, search_term: str, limit: int = 10) -> str:
        """Build query to search within JSONB formData"""
        return f"""
        SELECT 
            smd.id,
            smd.ob_number,
            smd."submissionDate",
            sm.name as module_name,
            smd."formData",
            smd.location,
            smd.urgency
        FROM sub_module_data smd
        LEFT JOIN sub_module sm ON smd."sub_moduleId" = sm.id
        WHERE smd."formData"::text ILIKE '%{search_term}%'
        OR smd.narrative ILIKE '%{search_term}%'
        ORDER BY smd."submissionDate" DESC
        LIMIT {limit}
        """
    
    def execute_query(self, query: str) -> Any:
        """Execute query and return results"""
        try:
            return self.db.run(query)
        except Exception as e:
            print(f"Query execution error: {e}")
            return None

# Initialize query builder
query_builder = JSONBQueryBuilder(db, schema_manager)
print("✅ JSONB Query Builder initialized!")## 2. Smart JSONB Query Builder

This class builds intelligent SQL queries that can extract and interpret data from your JSONB `formData` columns.# Display available modules and their schemas
print("Available Occurrence Types and Sample Fields:")
print("=" * 60)

for module_id, schema in schema_manager.schemas.items():
    print(f"\\n{module_id}. {schema['name']}")
    print(f"   Description: {schema['description']}")
    print(f"   Total fields: {len(schema['fields'])}")
    
    # Show first few fields as example
    print("   Sample fields:")
    for field in schema['fields'][:3]:
        field_name = field.get('name', 'Unknown')
        field_type = field.get('type', 'text')
        print(f"     - {field_name} ({field_type})")
    
    if len(schema['fields']) > 3:
        print(f"     ... and {len(schema['fields']) - 3} more fields")class FieldSchemaManager:
    """Manages field schemas from sub_module table"""
    
    def __init__(self, db: SQLDatabase):
        self.db = db
        self.schemas = {}
        self.load_schemas()
    
    def load_schemas(self):
        """Load all field schemas from sub_module table"""
        query = "SELECT id, name, description, fields FROM sub_module"
        result = self.db.run(query)
        
        # Parse the result
        import ast
        rows = ast.literal_eval(result)
        
        for row in rows:
            module_id, name, description, fields_json = row
            if fields_json:
                self.schemas[module_id] = {
                    'name': name,
                    'description': description,
                    'fields': fields_json
                }
        
        print(f"Loaded schemas for {len(self.schemas)} modules")
    
    def get_field_info(self, module_id: int) -> Dict:
        """Get field information for a specific module"""
        return self.schemas.get(module_id, {})
    
    def get_all_field_names(self) -> List[str]:
        """Get all unique field names across all modules"""
        all_fields = set()
        for schema in self.schemas.values():
            for field in schema.get('fields', []):
                all_fields.add(field.get('name', ''))
        return list(all_fields)
    
    def create_field_description(self, module_id: int) -> str:
        """Create human-readable field descriptions for LLM"""
        schema = self.get_field_info(module_id)
        if not schema:
            return "No schema information available"
        
        description = f"Module: {schema['name']} - {schema['description']}\\n\\nFields:\\n"
        
        for field in schema.get('fields', []):
            field_name = field.get('name', 'Unknown')
            field_type = field.get('type', 'text')
            required = field.get('required', False)
            options = field.get('options', [])
            
            description += f"- {field_name} ({field_type})"
            if required:
                description += " [REQUIRED]"
            if options:
                description += f" Options: {', '.join(options[:5])}{'...' if len(options) > 5 else ''}"
            description += "\\n"
        
        return description

# Initialize the schema manager
schema_manager = FieldSchemaManager(db)
print("✅ Field Schema Manager initialized!")## 1. Field Schema Manager

This class extracts and manages the field schemas from your `sub_module` table. It understands what fields are available for each occurrence type (Arson, Theft, Death, etc.).# Initialize database connection and AI models
db_uri = f"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASSWORD']}@{os.environ['DB_HOST']}:{os.environ['DB_PORT']}/{os.environ['DB_NAME']}"

try:
    db = SQLDatabase.from_uri(db_uri)
    print("✅ Database connected successfully!")
    print(f"Available tables: {db.get_usable_table_names()}")
except Exception as e:
    print(f"❌ Database connection failed: {e}")

# Initialize LLM and embeddings
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0)
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

print("✅ LLM and embeddings initialized!")# Environment setup
os.environ['GOOGLE_API_KEY'] = 'AIzaSyDsJJu5oN0BQrEKvnotU6uYEl5Mxw9fiug'
os.environ['LANGSMITH_API_KEY'] = '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'
os.environ['LANGSMITH_TRACING'] = 'true'

# Database connection
os.environ['DB_HOST'] = 'localhost'
os.environ['DB_PORT'] = '5432'
os.environ['DB_NAME'] = 'obmain'
os.environ['DB_USER'] = 'myuser'
os.environ['DB_PASSWORD'] = 'Welcome123'

print("✅ Environment configured!")# Setup and imports
import os
import json
import pandas as pd
from typing import Dict, List, Any, Optional

# LangChain and AI imports
from langchain_community.utilities import SQLDatabase
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

print("✅ Libraries imported!")# Enhanced RAG System for JSONB Occurrence Data

This notebook shows how to make your JSONB occurrence data intelligently available to your LLM by understanding the dynamic field schemas and creating smart queries.